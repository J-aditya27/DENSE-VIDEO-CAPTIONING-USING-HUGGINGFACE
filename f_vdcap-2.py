# -*- coding: utf-8 -*-
"""f_vdcap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k2JDUPOByYvUU5WOhtHgCgZWl--QpPYl
"""

!pip install transformers accelerate opencv-python pillow --quiet

import cv2
import numpy as np
import torch
import urllib.request
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

# STEP 4: Upload Your Own Video File
from google.colab import files

uploaded = files.upload()
video_path = list(uploaded.keys())[0]
print(f"Uploaded video: {video_path}")

# STEP 5: Process Video and Generate Captions
cap = cv2.VideoCapture(video_path)
fps = int(cap.get(cv2.CAP_PROP_FPS))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
out = cv2.VideoWriter('output_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

frame_count = 0
print("Generating captions frame by frame...")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    if frame_count % int(fps) == 0:  # Generate caption every 1 second
        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(img)
        inputs = processor(images=pil_image, return_tensors="pt").to(device)
        out_caption = model.generate(**inputs)
        caption = processor.decode(out_caption[0], skip_special_tokens=True)
        cv2.putText(frame, caption, (10, height - 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

    out.write(frame)
    frame_count += 1

cap.release()
out.release()
print("Video captioning complete!")

from google.colab import files
files.download('output_video.mp4')